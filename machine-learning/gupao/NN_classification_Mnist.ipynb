{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e44403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c36cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"initial path \"\"\"\n",
    "DATAPATH = Path(\"F:\\DL\\Mnist_data\")\n",
    "PATH = DATAPATH / \"mnist\"\n",
    "FILENAME = \"mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de05f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f83ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c9aa4e48b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc9600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17048\\3222872844.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30a983af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.2200, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "def model(xb):\n",
    "    return xb.mm(weights) + bias\n",
    "bs = 64\n",
    "xb = x_train[0:bs]\n",
    "yb = y_train[0:bs]\n",
    "weights = torch.randn([784, 10], dtype=torch.float, requires_grad=True)\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a04d53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 128)  # 输入784, 特征h1: 128\n",
    "        self.hidden2 = nn.Linear(128, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):  # x a batch data\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13eb1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_NN(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "hidden1.weight Parameter containing:\n",
      "tensor([[ 0.0208, -0.0306,  0.0193,  ...,  0.0043, -0.0280,  0.0326],\n",
      "        [-0.0197, -0.0014,  0.0231,  ...,  0.0285,  0.0343,  0.0020],\n",
      "        [ 0.0030,  0.0088,  0.0077,  ..., -0.0235, -0.0048, -0.0161],\n",
      "        ...,\n",
      "        [-0.0114, -0.0328,  0.0125,  ...,  0.0352,  0.0032,  0.0086],\n",
      "        [ 0.0073,  0.0098,  0.0338,  ..., -0.0137,  0.0302, -0.0294],\n",
      "        [ 0.0164,  0.0177, -0.0035,  ...,  0.0246, -0.0051,  0.0283]],\n",
      "       requires_grad=True) torch.Size([128, 784])\n",
      "hidden1.bias Parameter containing:\n",
      "tensor([-0.0096,  0.0018,  0.0323,  0.0357, -0.0073,  0.0152,  0.0167, -0.0114,\n",
      "        -0.0286, -0.0206, -0.0327, -0.0344, -0.0016,  0.0067, -0.0239,  0.0129,\n",
      "         0.0027, -0.0037,  0.0233,  0.0159,  0.0317, -0.0124,  0.0050, -0.0180,\n",
      "         0.0036,  0.0298,  0.0019, -0.0229, -0.0241, -0.0136, -0.0235, -0.0265,\n",
      "         0.0300, -0.0014, -0.0124,  0.0310, -0.0313,  0.0357, -0.0118,  0.0293,\n",
      "        -0.0068,  0.0025,  0.0125, -0.0209,  0.0240,  0.0055, -0.0131,  0.0195,\n",
      "        -0.0061,  0.0106,  0.0273,  0.0289, -0.0260, -0.0192,  0.0053, -0.0278,\n",
      "        -0.0297,  0.0132, -0.0334,  0.0317, -0.0343, -0.0285, -0.0243, -0.0034,\n",
      "         0.0151, -0.0028,  0.0281, -0.0262, -0.0038,  0.0307, -0.0135,  0.0351,\n",
      "         0.0091,  0.0258, -0.0210,  0.0159,  0.0134,  0.0349,  0.0070, -0.0306,\n",
      "        -0.0063, -0.0186,  0.0355,  0.0132, -0.0104, -0.0097,  0.0135, -0.0076,\n",
      "        -0.0001,  0.0238, -0.0298, -0.0055, -0.0009, -0.0146, -0.0226,  0.0026,\n",
      "         0.0142,  0.0179, -0.0099, -0.0077, -0.0192,  0.0091, -0.0190,  0.0337,\n",
      "         0.0265, -0.0071, -0.0111,  0.0197, -0.0002, -0.0345,  0.0008, -0.0203,\n",
      "         0.0017, -0.0209,  0.0317,  0.0083, -0.0261,  0.0251, -0.0029, -0.0210,\n",
      "         0.0056,  0.0318,  0.0172, -0.0109,  0.0170, -0.0350,  0.0166, -0.0109],\n",
      "       requires_grad=True) torch.Size([128])\n",
      "hidden2.weight Parameter containing:\n",
      "tensor([[-0.0716, -0.0538,  0.0010,  ..., -0.0344, -0.0851,  0.0576],\n",
      "        [-0.0672, -0.0755, -0.0049,  ..., -0.0364, -0.0629, -0.0272],\n",
      "        [-0.0396, -0.0268, -0.0252,  ..., -0.0731, -0.0732,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0827, -0.0215, -0.0143,  ..., -0.0055,  0.0638, -0.0174],\n",
      "        [ 0.0088,  0.0266,  0.0034,  ..., -0.0343, -0.0529, -0.0200],\n",
      "        [ 0.0750, -0.0656, -0.0040,  ..., -0.0012, -0.0417, -0.0645]],\n",
      "       requires_grad=True) torch.Size([256, 128])\n",
      "hidden2.bias Parameter containing:\n",
      "tensor([-0.0446, -0.0051, -0.0283, -0.0797,  0.0557, -0.0412, -0.0774,  0.0096,\n",
      "        -0.0168,  0.0552, -0.0098,  0.0355, -0.0458, -0.0555, -0.0274, -0.0766,\n",
      "         0.0627, -0.0872,  0.0706,  0.0309,  0.0749, -0.0426, -0.0130, -0.0735,\n",
      "         0.0229, -0.0868, -0.0248, -0.0467, -0.0454,  0.0252, -0.0729, -0.0671,\n",
      "        -0.0805,  0.0876, -0.0500, -0.0317, -0.0301,  0.0807, -0.0772,  0.0127,\n",
      "        -0.0600,  0.0773,  0.0119, -0.0535,  0.0478,  0.0329, -0.0796, -0.0007,\n",
      "         0.0377,  0.0268, -0.0400, -0.0654, -0.0616,  0.0381, -0.0743, -0.0626,\n",
      "        -0.0643, -0.0557, -0.0044,  0.0400,  0.0026, -0.0594, -0.0740, -0.0568,\n",
      "         0.0546,  0.0804,  0.0821,  0.0395, -0.0639,  0.0638,  0.0826,  0.0501,\n",
      "         0.0258,  0.0659, -0.0758,  0.0540, -0.0165,  0.0828,  0.0347, -0.0208,\n",
      "         0.0434,  0.0822,  0.0505,  0.0558,  0.0818, -0.0486,  0.0851,  0.0488,\n",
      "        -0.0254,  0.0473,  0.0209, -0.0496, -0.0546, -0.0133,  0.0506,  0.0343,\n",
      "         0.0653,  0.0569, -0.0565, -0.0527,  0.0279, -0.0848,  0.0025,  0.0225,\n",
      "        -0.0431,  0.0326,  0.0751, -0.0137,  0.0800, -0.0386, -0.0055, -0.0190,\n",
      "        -0.0075, -0.0293,  0.0249, -0.0496,  0.0418,  0.0157,  0.0487,  0.0536,\n",
      "        -0.0505, -0.0284, -0.0046, -0.0719, -0.0083,  0.0320, -0.0857, -0.0244,\n",
      "         0.0665,  0.0311, -0.0311,  0.0640,  0.0338,  0.0695, -0.0139,  0.0656,\n",
      "         0.0697,  0.0397, -0.0315,  0.0467, -0.0381, -0.0152, -0.0615,  0.0472,\n",
      "         0.0301, -0.0517, -0.0016, -0.0509,  0.0769,  0.0661, -0.0318,  0.0706,\n",
      "         0.0166,  0.0479, -0.0463,  0.0058, -0.0823, -0.0863,  0.0044,  0.0778,\n",
      "         0.0131,  0.0544, -0.0015, -0.0403, -0.0806, -0.0832, -0.0670,  0.0859,\n",
      "        -0.0246,  0.0158,  0.0489, -0.0653,  0.0175,  0.0674,  0.0369, -0.0497,\n",
      "         0.0482, -0.0154, -0.0531, -0.0685,  0.0867, -0.0187,  0.0441,  0.0713,\n",
      "         0.0863, -0.0549, -0.0652,  0.0372, -0.0309,  0.0120, -0.0478, -0.0247,\n",
      "        -0.0197,  0.0077,  0.0854,  0.0296, -0.0239, -0.0849,  0.0587,  0.0322,\n",
      "         0.0139,  0.0642,  0.0483,  0.0700, -0.0141, -0.0661,  0.0695, -0.0776,\n",
      "         0.0820, -0.0234,  0.0459, -0.0379,  0.0558,  0.0357,  0.0154,  0.0234,\n",
      "        -0.0303, -0.0154,  0.0162,  0.0467, -0.0609, -0.0528,  0.0037,  0.0660,\n",
      "        -0.0427,  0.0115, -0.0369, -0.0285, -0.0007, -0.0661, -0.0339,  0.0833,\n",
      "        -0.0870,  0.0048, -0.0818,  0.0466,  0.0098, -0.0256,  0.0585,  0.0872,\n",
      "         0.0254,  0.0096, -0.0522, -0.0734,  0.0779, -0.0727, -0.0526, -0.0216,\n",
      "        -0.0360,  0.0866,  0.0145,  0.0691, -0.0407,  0.0553,  0.0504,  0.0706],\n",
      "       requires_grad=True) torch.Size([256])\n",
      "out.weight Parameter containing:\n",
      "tensor([[-0.0289, -0.0328, -0.0380,  ..., -0.0476, -0.0135,  0.0214],\n",
      "        [ 0.0149,  0.0102, -0.0293,  ...,  0.0185,  0.0258, -0.0050],\n",
      "        [-0.0347,  0.0184, -0.0559,  ..., -0.0353,  0.0406,  0.0455],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0301,  0.0448,  ...,  0.0240,  0.0049,  0.0482],\n",
      "        [-0.0193,  0.0320, -0.0195,  ..., -0.0185, -0.0108, -0.0560],\n",
      "        [ 0.0002, -0.0447,  0.0140,  ...,  0.0282, -0.0467,  0.0325]],\n",
      "       requires_grad=True) torch.Size([10, 256])\n",
      "out.bias Parameter containing:\n",
      "tensor([-0.0038, -0.0517, -0.0481,  0.0338, -0.0222, -0.0504, -0.0003,  0.0346,\n",
      "         0.0296, -0.0286], requires_grad=True) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = Mnist_NN()\n",
    "print(net)\n",
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter, parameter.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f7aaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774ac5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_dl, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa3d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization :\n",
    "def get_model():\n",
    "    model = Mnist_NN()\n",
    "    return model, optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67746cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "776b93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for step in range(steps):  # steps is epoch.\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]) # 打包解包一一匹配为单个数据.\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(\"currently step:\"+str(step), \"validate loss:\"+str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94a2cccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently step:0 validate loss:0.18235376028120517\n",
      "currently step:1 validate loss:0.13997925418764354\n",
      "currently step:2 validate loss:0.12318924208283424\n",
      "currently step:3 validate loss:0.10824957657754421\n",
      "currently step:4 validate loss:0.10309406057801097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(5, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
